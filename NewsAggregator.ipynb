{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpW2ameNIf742O8VMwfbsV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nileshkumar2368/NewsAggregator/blob/main/NewsAggregator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn scikit-learn nltk pandas numpy pydantic joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIOA9pj9TDmO",
        "outputId": "435692fb-3c45-47d1-e00b-35919afacfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.10.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.8 starlette-0.45.3 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQt3myFeSse3",
        "outputId": "62824e56-ae3e-46ac-fa2f-76765858c578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Your provided dataset\n",
        "data = {\n",
        "  \"articles\": [\n",
        "    {\"id\": 1, \"headline\": \"NASA confirms water on Mars\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 2, \"headline\": \"You won’t believe what this celebrity did at a party!\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 3, \"headline\": \"Government officials caught in secret meeting about UFOs!\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 4, \"headline\": \"New study finds potential cure for cancer\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 5, \"headline\": \"The TRUTH about 5G towers they don’t want you to know!\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 6, \"headline\": \"Stock market crashes due to shocking reason!\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 7, \"headline\": \"Scientists predict Earth’s magnetic poles will shift soon\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 8, \"headline\": \"Did the government hide evidence of alien life?\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 9, \"headline\": \"Elections were rigged using secret hacking techniques\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 10, \"headline\": \"This one food will make you lose weight instantly!\", \"category\": \"Clickbait\"},\n",
        "\n",
        "    {\"id\": 11, \"headline\": \"Climate change is accelerating faster than expected\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 12, \"headline\": \"  \", \"category\": \"Unknown\"},\n",
        "    {\"id\": 13, \"headline\": \"THE GOVERNMENT IS WATCHING YOU!\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 14, \"headline\": \"5G is dangerous!!!\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 15, \"headline\": \"Stock Market Sees Biggest Drop in 10 Years\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 16, \"headline\": \"Massive UFO sighting reported over New York\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 17, \"headline\": \"President denies all allegations in press conference\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 18, \"headline\": \"Can drinking lemon water cure cancer? Experts weigh in\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 19, \"headline\": \"Doctors warn against the latest TikTok health craze\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 20, \"headline\": \"BREAKING: New world order forming, experts claim!\", \"category\": \"Conspiracy\"},\n",
        "\n",
        "    {\"id\": 100, \"headline\": \"Study: People who wake up early are more successful\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 101, \"headline\": \"Secret societies control the world economy\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 102, \"headline\": \"Do vaccines REALLY work? A shocking revelation!\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 103, \"headline\": \"Is sugar more addictive than cocaine? Scientists debate\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 104, \"headline\": \"Tech giant CEO found dead under mysterious circumstances\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 105, \"headline\": \"Government to announce strict new surveillance policies\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 106, \"headline\": \"Exclusive: Inside the top-secret military base that doesn’t exist\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 107, \"headline\": \"We tried this viral TikTok trend—here’s what happened!\", \"category\": \"Clickbait\"},\n",
        "\n",
        "    {\"id\": 200, \"headline\": \"Scientists discover bacteria that can digest plastic\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 201, \"headline\": \"Aliens spotted in Area 51? Here’s what we know!\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 202, \"headline\": \"You’re eating THIS toxic ingredient every day!\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 203, \"headline\": \"Astronomers detect radio signals from deep space\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 204, \"headline\": \"Governments are hiding the truth about the moon landing!\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 205, \"headline\": \"Simple tricks to get rich overnight!\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 206, \"headline\": \"Shocking new health benefits of eating chocolate\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 207, \"headline\": \"The real reason billionaires don’t pay taxes\", \"category\": \"Conspiracy\"},\n",
        "\n",
        "    {\"id\": 300, \"headline\": \"Researchers develop new AI that can detect cancer early\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 301, \"headline\": \"EXPOSED: What doctors aren’t telling you about prescription drugs!\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 302, \"headline\": \"Can meditation make you smarter? Here’s what science says\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 303, \"headline\": \"Is social media making you depressed? Experts say yes\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 304, \"headline\": \"Scientists warn about hidden dangers of climate change\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 305, \"headline\": \"You won’t believe what this celebrity did on live TV!\", \"category\": \"Clickbait\"},\n",
        "\n",
        "    {\"id\": 400, \"headline\": \"BREAKING: New technology could make humans live forever\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 401, \"headline\": \"Doctors discover shocking link between diet and mental health\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 402, \"headline\": \"New world order? Leaked documents reveal hidden agenda\", \"category\": \"Conspiracy\"},\n",
        "    {\"id\": 403, \"headline\": \"The one exercise that burns belly fat instantly!\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 404, \"headline\": \"Government funding new AI research for national security\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 405, \"headline\": \"How a 12-year-old made millions with cryptocurrency\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 406, \"headline\": \"BREAKING: AI robot gains consciousness?\", \"category\": \"Conspiracy\"},\n",
        "\n",
        "    {\"id\": 500, \"headline\": \"Experts warn about the hidden dangers of processed foods\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 501, \"headline\": \"Elon Musk reveals shocking plans for Mars colonization\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 502, \"headline\": \"Billionaire donates entire fortune to climate change research\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 503, \"headline\": \"The shocking truth about fast food you need to know!\", \"category\": \"Clickbait\"},\n",
        "    {\"id\": 504, \"headline\": \"Scientists develop self-healing materials for construction\", \"category\": \"Legitimate\"},\n",
        "    {\"id\": 505, \"headline\": \"New breakthrough in quantum computing announced\", \"category\": \"Legitimate\"}\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "with open('news_headlines_large.json', 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "print(\"Dataset saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    # Load the dataset\n",
        "    with open('news_headlines_large.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data['articles'])\n",
        "\n",
        "    # Create numerical labels\n",
        "    category_map = {\n",
        "        'Legitimate': 0,\n",
        "        'Clickbait': 1,\n",
        "        'Conspiracy': 2\n",
        "    }\n",
        "\n",
        "    # Convert categories to numerical labels\n",
        "    df['label'] = df['category'].map(category_map)\n",
        "\n",
        "    # Remove any rows with missing headlines\n",
        "    df = df[df['headline'].str.strip().str.len() > 0]\n",
        "\n",
        "    return df\n",
        "\n",
        "def train_model():\n",
        "    # Load and prepare data\n",
        "    df = load_and_prepare_data()\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df['headline'],\n",
        "        df['label'],\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create and fit the vectorizer\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train the model\n",
        "    classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = classifier.predict(X_test_vectorized)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred,\n",
        "          target_names=['Legitimate', 'Clickbait', 'Conspiracy']))\n",
        "\n",
        "    # Save the model and vectorizer\n",
        "    joblib.dump(classifier, 'classifier.joblib')\n",
        "    joblib.dump(vectorizer, 'vectorizer.joblib')\n",
        "\n",
        "    return classifier, vectorizer\n",
        "\n",
        "# Train the model\n",
        "print(\"Training model...\")\n",
        "classifier, vectorizer = train_model()\n",
        "print(\"\\nModel training completed and saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-XHRXnoS7dO",
        "outputId": "e671482f-cbd3-4352-cdda-fb26ea850ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Legitimate       0.64      1.00      0.78         7\n",
            "   Clickbait       0.00      0.00      0.00         3\n",
            "  Conspiracy       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.64        11\n",
            "   macro avg       0.21      0.33      0.26        11\n",
            "weighted avg       0.40      0.64      0.49        11\n",
            "\n",
            "\n",
            "Model training completed and saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import uvicorn\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import nest_asyncio\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "app = FastAPI(title=\"News Aggregator API\")\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Load the trained model and vectorizer\n",
        "loaded_classifier = joblib.load('classifier.joblib')\n",
        "loaded_vectorizer = joblib.load('vectorizer.joblib')\n",
        "\n",
        "class Article(BaseModel):\n",
        "    headline: str\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "async def analyze_headline(article: Article):\n",
        "    try:\n",
        "        # Get features\n",
        "        features = loaded_vectorizer.transform([article.headline])\n",
        "\n",
        "        # Get prediction and probability\n",
        "        category_pred = loaded_classifier.predict(features)[0]\n",
        "        probabilities = loaded_classifier.predict_proba(features)[0]\n",
        "\n",
        "        # Map numerical prediction to category\n",
        "        categories = ['Legitimate', 'Clickbait', 'Conspiracy']\n",
        "        category = categories[category_pred]\n",
        "\n",
        "        # Calculate confidence score\n",
        "        confidence = float(probabilities[category_pred] * 100)\n",
        "\n",
        "        # Detect suspicious words\n",
        "        suspicious_words = set(['conspiracy', 'secret', 'shocking', 'exposed', 'truth',\n",
        "                              'coverup', 'hidden', 'they', 'them', 'control'])\n",
        "        tokens = word_tokenize(article.headline.lower())\n",
        "        found_suspicious = [word for word in tokens if word in suspicious_words]\n",
        "\n",
        "        return {\n",
        "            \"headline\": article.headline,\n",
        "            \"category\": category,\n",
        "            \"confidence\": confidence,\n",
        "            \"suspicious_words\": found_suspicious,\n",
        "            \"probabilities\": {\n",
        "                \"Legitimate\": float(probabilities[0] * 100),\n",
        "                \"Clickbait\": float(probabilities[1] * 100),\n",
        "                \"Conspiracy\": float(probabilities[2] * 100)\n",
        "            }\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Enable nested asyncio for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the app\n",
        "print(\"Starting the API server...\")\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w_GgW_jTX7t",
        "outputId": "787994a7-a575-4dac-bff6-77d5dfba31c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting the API server...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [239]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [239]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80TsD3E3UfcL",
        "outputId": "1a5c32d4-28bf-4648-87c7-e4970b53cece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import uvicorn\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "app = FastAPI(title=\"News Aggregator API\")\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Load the trained model and vectorizer\n",
        "loaded_classifier = joblib.load('classifier.joblib')\n",
        "loaded_vectorizer = joblib.load('vectorizer.joblib')\n",
        "\n",
        "class Article(BaseModel):\n",
        "    headline: str\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "async def analyze_headline(article: Article):\n",
        "    try:\n",
        "        # Get features\n",
        "        features = loaded_vectorizer.transform([article.headline])\n",
        "\n",
        "        # Get prediction and probability\n",
        "        category_pred = loaded_classifier.predict(features)[0]\n",
        "        probabilities = loaded_classifier.predict_proba(features)[0]\n",
        "\n",
        "        # Map numerical prediction to category\n",
        "        categories = ['Legitimate', 'Clickbait', 'Conspiracy']\n",
        "        category = categories[category_pred]\n",
        "\n",
        "        # Calculate confidence score\n",
        "        confidence = float(probabilities[category_pred] * 100)\n",
        "\n",
        "        # Detect suspicious words\n",
        "        suspicious_words = set(['conspiracy', 'secret', 'shocking', 'exposed', 'truth',\n",
        "                              'coverup', 'hidden', 'they', 'them', 'control'])\n",
        "        tokens = word_tokenize(article.headline.lower())\n",
        "        found_suspicious = [word for word in tokens if word in suspicious_words]\n",
        "\n",
        "        return {\n",
        "            \"headline\": article.headline,\n",
        "            \"category\": category,\n",
        "            \"confidence\": confidence,\n",
        "            \"suspicious_words\": found_suspicious,\n",
        "            \"probabilities\": {\n",
        "                \"Legitimate\": float(probabilities[0] * 100),\n",
        "                \"Clickbait\": float(probabilities[1] * 100),\n",
        "                \"Conspiracy\": float(probabilities[2] * 100)\n",
        "            }\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Setup ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f'Public URL: {public_url}')\n",
        "\n",
        "# Enable nested asyncio for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the app\n",
        "print(\"Starting the API server...\")\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n6rxIry4Uov3",
        "outputId": "b2d70663-ce6e-4c91-d83d-d3a7346396ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-4' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ngrok ...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-02-23T10:20:33+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-02-23T10:20:33+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-02-23T10:20:33+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-56401f1b2c97>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Setup ngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Public URL: {public_url}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    429\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU91QHASUy7W",
        "outputId": "3145faea-d7c4-4689-af03-b6fb102c9606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.17.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.8)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.1 (from gradio)\n",
            "  Downloading gradio_client-1.7.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.1->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.1->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.17.1-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.1-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, safehttpx, gradio-client, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 ffmpy-0.5.0 gradio-5.17.1 gradio-client-1.7.1 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.7 safehttpx-0.1.6 semantic-version-2.10.0 tomlkit-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import json\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load and prepare data\n",
        "def load_and_prepare_data():\n",
        "    # Your dataset\n",
        "    data = {\n",
        "        \"articles\": [\n",
        "            {\"id\": 1, \"headline\": \"NASA confirms water on Mars\", \"category\": \"Legitimate\"},\n",
        "            {\"id\": 2, \"headline\": \"You won't believe what this celebrity did at a party!\", \"category\": \"Clickbait\"},\n",
        "            # ... (rest of your data)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data['articles'])\n",
        "\n",
        "    # Create numerical labels\n",
        "    category_map = {\n",
        "        'Legitimate': 0,\n",
        "        'Clickbait': 1,\n",
        "        'Conspiracy': 2\n",
        "    }\n",
        "\n",
        "    # Convert categories to numerical labels\n",
        "    df['label'] = df['category'].map(category_map)\n",
        "\n",
        "    # Remove any rows with missing headlines\n",
        "    df = df[df['headline'].str.strip().str.len() > 0]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Train model\n",
        "def train_model():\n",
        "    df = load_and_prepare_data()\n",
        "\n",
        "    # Create and fit the vectorizer\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "    X = vectorizer.fit_transform(df['headline'])\n",
        "    y = df['label']\n",
        "\n",
        "    # Train the model\n",
        "    classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    classifier.fit(X, y)\n",
        "\n",
        "    return vectorizer, classifier\n",
        "\n",
        "# Train the model\n",
        "vectorizer, classifier = train_model()\n",
        "\n",
        "# Define the analysis function\n",
        "def analyze_headline(headline):\n",
        "    if not headline.strip():\n",
        "        return \"Please enter a headline.\"\n",
        "\n",
        "    # Get features\n",
        "    features = vectorizer.transform([headline])\n",
        "\n",
        "    # Get prediction and probability\n",
        "    category_pred = classifier.predict(features)[0]\n",
        "    probabilities = classifier.predict_proba(features)[0]\n",
        "\n",
        "    # Map numerical prediction to category\n",
        "    categories = ['Legitimate', 'Clickbait', 'Conspiracy']\n",
        "    category = categories[category_pred]\n",
        "\n",
        "    # Calculate confidence\n",
        "    confidence = float(probabilities[category_pred] * 100)\n",
        "\n",
        "    # Detect suspicious words\n",
        "    suspicious_words = set(['conspiracy', 'secret', 'shocking', 'exposed', 'truth',\n",
        "                          'coverup', 'hidden', 'they', 'them', 'control'])\n",
        "    tokens = word_tokenize(headline.lower())\n",
        "    found_suspicious = [word for word in tokens if word in suspicious_words]\n",
        "\n",
        "    # Format results\n",
        "    result = f\"\"\"\n",
        "    Headline Analysis:\n",
        "    -----------------\n",
        "    Category: {category}\n",
        "    Confidence: {confidence:.2f}%\n",
        "\n",
        "    Probability Distribution:\n",
        "    - Legitimate: {probabilities[0]*100:.2f}%\n",
        "    - Clickbait: {probabilities[1]*100:.2f}%\n",
        "    - Conspiracy: {probabilities[2]*100:.2f}%\n",
        "\n",
        "    Suspicious Words Found: {', '.join(found_suspicious) if found_suspicious else 'None'}\n",
        "    \"\"\"\n",
        "\n",
        "    return result\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=analyze_headline,\n",
        "    inputs=gr.Textbox(label=\"Enter a news headline\"),\n",
        "    outputs=gr.Textbox(label=\"Analysis Results\"),\n",
        "    title=\"News Headline Analyzer\",\n",
        "    description=\"Enter a news headline to analyze its category and detect potential misleading content.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "r-30hfwPU3z6",
        "outputId": "68a93060-3b57-415a-dd17-11db6d415f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5387aa87cee64d5a19.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5387aa87cee64d5a19.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}